{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 7\n",
    "num_epochs = 500\n",
    "dropout_ratio = 0.2\n",
    "\n",
    "data_path = 'data/weather.csv'\n",
    "mechanism = 'mcar'\n",
    "method = 'uniform'\n",
    "\n",
    "test_size = 0.2\n",
    "use_cuda = True\n",
    "batch_size  = 64 # not in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path).values\n",
    "\n",
    "rows, cols = data.shape\n",
    "shuffled_index = np.random.permutation(rows)\n",
    "train_index = shuffled_index[:int(rows*(1-test_size))]\n",
    "test_index = shuffled_index[int(rows*(1-test_size)):]\n",
    "\n",
    "train_data = data[train_index, :]\n",
    "test_data = data[test_index, :]\n",
    "\n",
    "# standardized between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_method(raw_data, mechanism='mcar', method='uniform') :\n",
    "    \n",
    "    data = raw_data.copy()\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # missingness threshold\n",
    "    t = dropout_ratio\n",
    "    \n",
    "    if mechanism == 'mcar' :\n",
    "    \n",
    "        if method == 'uniform' :\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There are no such method\")\n",
    "            raise\n",
    "    \n",
    "    elif mechanism == 'mnar' :\n",
    "        \n",
    "        if method == 'uniform' :\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)\n",
    "            data[mask] = 0\n",
    "\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)*c\n",
    "            data[mask] = 0\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There is no such method\")\n",
    "            raise\n",
    "    \n",
    "    else :\n",
    "        print(\"Error : There is no such mechanism\")\n",
    "        raise\n",
    "        \n",
    "    return data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_data, mask = missing_method(test_data, mechanism=mechanism, method=method)\n",
    "\n",
    "missed_data = torch.from_numpy(missed_data).float()\n",
    "train_data = torch.from_numpy(train_data).float()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.drop_out = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*0, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*3)\n",
    "        )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim+theta*3, dim+theta*2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*2, dim+theta*1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim+theta*1, dim+theta*0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.dim)\n",
    "        x_missed = self.drop_out(x)\n",
    "        \n",
    "        z = self.encoder(x_missed)\n",
    "        out = self.decoder(z)\n",
    "        \n",
    "        out = out.view(-1, self.dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(dim=cols).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.99, lr=0.01, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], lter [4/9], Loss: 0.159280\n",
      "Epoch [1/500], lter [8/9], Loss: 0.136529\n",
      "Epoch [2/500], lter [4/9], Loss: 0.077719\n",
      "Epoch [2/500], lter [8/9], Loss: 0.064275\n",
      "Epoch [3/500], lter [4/9], Loss: 0.093592\n",
      "Epoch [3/500], lter [8/9], Loss: 0.121659\n",
      "Epoch [4/500], lter [4/9], Loss: 0.103500\n",
      "Epoch [4/500], lter [8/9], Loss: 0.082784\n",
      "Epoch [5/500], lter [4/9], Loss: 0.057528\n",
      "Epoch [5/500], lter [8/9], Loss: 0.081685\n",
      "Epoch [6/500], lter [4/9], Loss: 0.079505\n",
      "Epoch [6/500], lter [8/9], Loss: 0.069537\n",
      "Epoch [7/500], lter [4/9], Loss: 0.064684\n",
      "Epoch [7/500], lter [8/9], Loss: 0.055559\n",
      "Epoch [8/500], lter [4/9], Loss: 0.051290\n",
      "Epoch [8/500], lter [8/9], Loss: 0.048414\n",
      "Epoch [9/500], lter [4/9], Loss: 0.037703\n",
      "Epoch [9/500], lter [8/9], Loss: 0.035133\n",
      "Epoch [10/500], lter [4/9], Loss: 0.044334\n",
      "Epoch [10/500], lter [8/9], Loss: 0.063946\n",
      "Epoch [11/500], lter [4/9], Loss: 0.034872\n",
      "Epoch [11/500], lter [8/9], Loss: 0.043519\n",
      "Epoch [12/500], lter [4/9], Loss: 0.035274\n",
      "Epoch [12/500], lter [8/9], Loss: 0.033878\n",
      "Epoch [13/500], lter [4/9], Loss: 0.030043\n",
      "Epoch [13/500], lter [8/9], Loss: 0.029589\n",
      "Epoch [14/500], lter [4/9], Loss: 0.036258\n",
      "Epoch [14/500], lter [8/9], Loss: 0.031773\n",
      "Epoch [15/500], lter [4/9], Loss: 0.041058\n",
      "Epoch [15/500], lter [8/9], Loss: 0.041200\n",
      "Epoch [16/500], lter [4/9], Loss: 0.037850\n",
      "Epoch [16/500], lter [8/9], Loss: 0.030621\n",
      "Epoch [17/500], lter [4/9], Loss: 0.039296\n",
      "Epoch [17/500], lter [8/9], Loss: 0.021816\n",
      "Epoch [18/500], lter [4/9], Loss: 0.026827\n",
      "Epoch [18/500], lter [8/9], Loss: 0.035234\n",
      "Epoch [19/500], lter [4/9], Loss: 0.026300\n",
      "Epoch [19/500], lter [8/9], Loss: 0.044295\n",
      "Epoch [20/500], lter [4/9], Loss: 0.025037\n",
      "Epoch [20/500], lter [8/9], Loss: 0.046454\n",
      "Epoch [21/500], lter [4/9], Loss: 0.036670\n",
      "Epoch [21/500], lter [8/9], Loss: 0.028603\n",
      "Epoch [22/500], lter [4/9], Loss: 0.029025\n",
      "Epoch [22/500], lter [8/9], Loss: 0.033000\n",
      "Epoch [23/500], lter [4/9], Loss: 0.034206\n",
      "Epoch [23/500], lter [8/9], Loss: 0.036228\n",
      "Epoch [24/500], lter [4/9], Loss: 0.045220\n",
      "Epoch [24/500], lter [8/9], Loss: 0.040158\n",
      "Epoch [25/500], lter [4/9], Loss: 0.034213\n",
      "Epoch [25/500], lter [8/9], Loss: 0.023822\n",
      "Epoch [26/500], lter [4/9], Loss: 0.036191\n",
      "Epoch [26/500], lter [8/9], Loss: 0.026145\n",
      "Epoch [27/500], lter [4/9], Loss: 0.018102\n",
      "Epoch [27/500], lter [8/9], Loss: 0.019999\n",
      "Epoch [28/500], lter [4/9], Loss: 0.047634\n",
      "Epoch [28/500], lter [8/9], Loss: 0.042377\n",
      "Epoch [29/500], lter [4/9], Loss: 0.019979\n",
      "Epoch [29/500], lter [8/9], Loss: 0.027466\n",
      "Epoch [30/500], lter [4/9], Loss: 0.025039\n",
      "Epoch [30/500], lter [8/9], Loss: 0.021282\n",
      "Epoch [31/500], lter [4/9], Loss: 0.035158\n",
      "Epoch [31/500], lter [8/9], Loss: 0.023819\n",
      "Epoch [32/500], lter [4/9], Loss: 0.021731\n",
      "Epoch [32/500], lter [8/9], Loss: 0.023024\n",
      "Epoch [33/500], lter [4/9], Loss: 0.029622\n",
      "Epoch [33/500], lter [8/9], Loss: 0.023233\n",
      "Epoch [34/500], lter [4/9], Loss: 0.019581\n",
      "Epoch [34/500], lter [8/9], Loss: 0.040121\n",
      "Epoch [35/500], lter [4/9], Loss: 0.020535\n",
      "Epoch [35/500], lter [8/9], Loss: 0.019237\n",
      "Epoch [36/500], lter [4/9], Loss: 0.031961\n",
      "Epoch [36/500], lter [8/9], Loss: 0.038037\n",
      "Epoch [37/500], lter [4/9], Loss: 0.023753\n",
      "Epoch [37/500], lter [8/9], Loss: 0.018330\n",
      "Epoch [38/500], lter [4/9], Loss: 0.026795\n",
      "Epoch [38/500], lter [8/9], Loss: 0.039485\n",
      "Epoch [39/500], lter [4/9], Loss: 0.029306\n",
      "Epoch [39/500], lter [8/9], Loss: 0.020690\n",
      "Epoch [40/500], lter [4/9], Loss: 0.020166\n",
      "Epoch [40/500], lter [8/9], Loss: 0.035723\n",
      "Epoch [41/500], lter [4/9], Loss: 0.031912\n",
      "Epoch [41/500], lter [8/9], Loss: 0.023881\n",
      "Epoch [42/500], lter [4/9], Loss: 0.023101\n",
      "Epoch [42/500], lter [8/9], Loss: 0.029445\n",
      "Epoch [43/500], lter [4/9], Loss: 0.018432\n",
      "Epoch [43/500], lter [8/9], Loss: 0.035886\n",
      "Epoch [44/500], lter [4/9], Loss: 0.025886\n",
      "Epoch [44/500], lter [8/9], Loss: 0.030070\n",
      "Epoch [45/500], lter [4/9], Loss: 0.039768\n",
      "Epoch [45/500], lter [8/9], Loss: 0.035137\n",
      "Epoch [46/500], lter [4/9], Loss: 0.024383\n",
      "Epoch [46/500], lter [8/9], Loss: 0.025910\n",
      "Epoch [47/500], lter [4/9], Loss: 0.014611\n",
      "Epoch [47/500], lter [8/9], Loss: 0.022497\n",
      "Epoch [48/500], lter [4/9], Loss: 0.030584\n",
      "Epoch [48/500], lter [8/9], Loss: 0.028787\n",
      "Epoch [49/500], lter [4/9], Loss: 0.024603\n",
      "Epoch [49/500], lter [8/9], Loss: 0.025040\n",
      "Epoch [50/500], lter [4/9], Loss: 0.020772\n",
      "Epoch [50/500], lter [8/9], Loss: 0.018539\n",
      "Epoch [51/500], lter [4/9], Loss: 0.018302\n",
      "Epoch [51/500], lter [8/9], Loss: 0.034488\n",
      "Epoch [52/500], lter [4/9], Loss: 0.041339\n",
      "Epoch [52/500], lter [8/9], Loss: 0.024781\n",
      "Epoch [53/500], lter [4/9], Loss: 0.037428\n",
      "Epoch [53/500], lter [8/9], Loss: 0.022059\n",
      "Epoch [54/500], lter [4/9], Loss: 0.027822\n",
      "Epoch [54/500], lter [8/9], Loss: 0.033519\n",
      "Epoch [55/500], lter [4/9], Loss: 0.024950\n",
      "Epoch [55/500], lter [8/9], Loss: 0.041073\n",
      "Epoch [56/500], lter [4/9], Loss: 0.022605\n",
      "Epoch [56/500], lter [8/9], Loss: 0.025640\n",
      "Epoch [57/500], lter [4/9], Loss: 0.019079\n",
      "Epoch [57/500], lter [8/9], Loss: 0.023674\n",
      "Epoch [58/500], lter [4/9], Loss: 0.027567\n",
      "Epoch [58/500], lter [8/9], Loss: 0.021159\n",
      "Epoch [59/500], lter [4/9], Loss: 0.029332\n",
      "Epoch [59/500], lter [8/9], Loss: 0.017412\n",
      "Epoch [60/500], lter [4/9], Loss: 0.024142\n",
      "Epoch [60/500], lter [8/9], Loss: 0.028993\n",
      "Epoch [61/500], lter [4/9], Loss: 0.018599\n",
      "Epoch [61/500], lter [8/9], Loss: 0.024007\n",
      "Epoch [62/500], lter [4/9], Loss: 0.033291\n",
      "Epoch [62/500], lter [8/9], Loss: 0.026846\n",
      "Epoch [63/500], lter [4/9], Loss: 0.023817\n",
      "Epoch [63/500], lter [8/9], Loss: 0.023238\n",
      "Epoch [64/500], lter [4/9], Loss: 0.016493\n",
      "Epoch [64/500], lter [8/9], Loss: 0.020153\n",
      "Epoch [65/500], lter [4/9], Loss: 0.027790\n",
      "Epoch [65/500], lter [8/9], Loss: 0.021191\n",
      "Epoch [66/500], lter [4/9], Loss: 0.017438\n",
      "Epoch [66/500], lter [8/9], Loss: 0.012376\n",
      "Epoch [67/500], lter [4/9], Loss: 0.027198\n",
      "Epoch [67/500], lter [8/9], Loss: 0.026318\n",
      "Epoch [68/500], lter [4/9], Loss: 0.017908\n",
      "Epoch [68/500], lter [8/9], Loss: 0.019748\n",
      "Epoch [69/500], lter [4/9], Loss: 0.011023\n",
      "Epoch [69/500], lter [8/9], Loss: 0.009522\n",
      "Epoch [70/500], lter [4/9], Loss: 0.014271\n",
      "Epoch [70/500], lter [8/9], Loss: 0.011799\n",
      "Epoch [71/500], lter [4/9], Loss: 0.020510\n",
      "Epoch [71/500], lter [8/9], Loss: 0.013533\n",
      "Epoch [72/500], lter [4/9], Loss: 0.011209\n",
      "Epoch [72/500], lter [8/9], Loss: 0.034705\n",
      "Epoch [73/500], lter [4/9], Loss: 0.024540\n",
      "Epoch [73/500], lter [8/9], Loss: 0.019634\n",
      "Epoch [74/500], lter [4/9], Loss: 0.021793\n",
      "Epoch [74/500], lter [8/9], Loss: 0.011770\n",
      "Epoch [75/500], lter [4/9], Loss: 0.028128\n",
      "Epoch [75/500], lter [8/9], Loss: 0.013323\n",
      "Epoch [76/500], lter [4/9], Loss: 0.017605\n",
      "Epoch [76/500], lter [8/9], Loss: 0.011080\n",
      "Epoch [77/500], lter [4/9], Loss: 0.009055\n",
      "Epoch [77/500], lter [8/9], Loss: 0.007414\n",
      "Epoch [78/500], lter [4/9], Loss: 0.017276\n",
      "Epoch [78/500], lter [8/9], Loss: 0.016280\n",
      "Epoch [79/500], lter [4/9], Loss: 0.019586\n",
      "Epoch [79/500], lter [8/9], Loss: 0.011109\n",
      "Epoch [80/500], lter [4/9], Loss: 0.016512\n",
      "Epoch [80/500], lter [8/9], Loss: 0.013084\n",
      "Epoch [81/500], lter [4/9], Loss: 0.015922\n",
      "Epoch [81/500], lter [8/9], Loss: 0.018177\n",
      "Epoch [82/500], lter [4/9], Loss: 0.006317\n",
      "Epoch [82/500], lter [8/9], Loss: 0.016164\n",
      "Epoch [83/500], lter [4/9], Loss: 0.006830\n",
      "Epoch [83/500], lter [8/9], Loss: 0.007887\n",
      "Epoch [84/500], lter [4/9], Loss: 0.006795\n",
      "Epoch [84/500], lter [8/9], Loss: 0.009423\n",
      "Epoch [85/500], lter [4/9], Loss: 0.026338\n",
      "Epoch [85/500], lter [8/9], Loss: 0.007573\n",
      "Epoch [86/500], lter [4/9], Loss: 0.009218\n",
      "Epoch [86/500], lter [8/9], Loss: 0.027854\n",
      "Epoch [87/500], lter [4/9], Loss: 0.012303\n",
      "Epoch [87/500], lter [8/9], Loss: 0.020710\n",
      "Epoch [88/500], lter [4/9], Loss: 0.006213\n",
      "Epoch [88/500], lter [8/9], Loss: 0.014713\n",
      "Epoch [89/500], lter [4/9], Loss: 0.007545\n",
      "Epoch [89/500], lter [8/9], Loss: 0.012097\n",
      "Epoch [90/500], lter [4/9], Loss: 0.020267\n",
      "Epoch [90/500], lter [8/9], Loss: 0.006480\n",
      "Epoch [91/500], lter [4/9], Loss: 0.016257\n",
      "Epoch [91/500], lter [8/9], Loss: 0.008616\n",
      "Epoch [92/500], lter [4/9], Loss: 0.019089\n",
      "Epoch [92/500], lter [8/9], Loss: 0.008830\n",
      "Epoch [93/500], lter [4/9], Loss: 0.010748\n",
      "Epoch [93/500], lter [8/9], Loss: 0.022796\n",
      "Epoch [94/500], lter [4/9], Loss: 0.008476\n",
      "Epoch [94/500], lter [8/9], Loss: 0.011976\n",
      "Epoch [95/500], lter [4/9], Loss: 0.010712\n",
      "Epoch [95/500], lter [8/9], Loss: 0.019741\n",
      "Epoch [96/500], lter [4/9], Loss: 0.012284\n",
      "Epoch [96/500], lter [8/9], Loss: 0.005529\n",
      "Epoch [97/500], lter [4/9], Loss: 0.007807\n",
      "Epoch [97/500], lter [8/9], Loss: 0.016383\n",
      "Epoch [98/500], lter [4/9], Loss: 0.013809\n",
      "Epoch [98/500], lter [8/9], Loss: 0.006147\n",
      "Epoch [99/500], lter [4/9], Loss: 0.012986\n",
      "Epoch [99/500], lter [8/9], Loss: 0.015834\n",
      "Epoch [100/500], lter [4/9], Loss: 0.013604\n",
      "Epoch [100/500], lter [8/9], Loss: 0.019190\n",
      "Epoch [101/500], lter [4/9], Loss: 0.011350\n",
      "Epoch [101/500], lter [8/9], Loss: 0.008459\n",
      "Epoch [102/500], lter [4/9], Loss: 0.009098\n",
      "Epoch [102/500], lter [8/9], Loss: 0.018011\n",
      "Epoch [103/500], lter [4/9], Loss: 0.007804\n",
      "Epoch [103/500], lter [8/9], Loss: 0.015767\n",
      "Epoch [104/500], lter [4/9], Loss: 0.014878\n",
      "Epoch [104/500], lter [8/9], Loss: 0.014768\n",
      "Epoch [105/500], lter [4/9], Loss: 0.013315\n",
      "Epoch [105/500], lter [8/9], Loss: 0.011199\n",
      "Epoch [106/500], lter [4/9], Loss: 0.005351\n",
      "Epoch [106/500], lter [8/9], Loss: 0.004828\n",
      "Epoch [107/500], lter [4/9], Loss: 0.021564\n",
      "Epoch [107/500], lter [8/9], Loss: 0.022517\n",
      "Epoch [108/500], lter [4/9], Loss: 0.014315\n",
      "Epoch [108/500], lter [8/9], Loss: 0.004877\n",
      "Epoch [109/500], lter [4/9], Loss: 0.008812\n",
      "Epoch [109/500], lter [8/9], Loss: 0.012831\n",
      "Epoch [110/500], lter [4/9], Loss: 0.012015\n",
      "Epoch [110/500], lter [8/9], Loss: 0.017585\n",
      "Epoch [111/500], lter [4/9], Loss: 0.014531\n",
      "Epoch [111/500], lter [8/9], Loss: 0.011715\n",
      "Epoch [112/500], lter [4/9], Loss: 0.006933\n",
      "Epoch [112/500], lter [8/9], Loss: 0.007883\n",
      "Epoch [113/500], lter [4/9], Loss: 0.009381\n",
      "Epoch [113/500], lter [8/9], Loss: 0.012504\n",
      "Epoch [114/500], lter [4/9], Loss: 0.009514\n",
      "Epoch [114/500], lter [8/9], Loss: 0.014607\n",
      "Epoch [115/500], lter [4/9], Loss: 0.014870\n",
      "Epoch [115/500], lter [8/9], Loss: 0.013966\n",
      "Epoch [116/500], lter [4/9], Loss: 0.012227\n",
      "Epoch [116/500], lter [8/9], Loss: 0.012322\n",
      "Epoch [117/500], lter [4/9], Loss: 0.011461\n",
      "Epoch [117/500], lter [8/9], Loss: 0.013104\n",
      "Epoch [118/500], lter [4/9], Loss: 0.005273\n",
      "Epoch [118/500], lter [8/9], Loss: 0.008309\n",
      "Epoch [119/500], lter [4/9], Loss: 0.004377\n",
      "Epoch [119/500], lter [8/9], Loss: 0.006729\n",
      "Epoch [120/500], lter [4/9], Loss: 0.011353\n",
      "Epoch [120/500], lter [8/9], Loss: 0.005961\n",
      "Epoch [121/500], lter [4/9], Loss: 0.016993\n",
      "Epoch [121/500], lter [8/9], Loss: 0.004711\n",
      "Epoch [122/500], lter [4/9], Loss: 0.009625\n",
      "Epoch [122/500], lter [8/9], Loss: 0.014580\n",
      "Epoch [123/500], lter [4/9], Loss: 0.008633\n",
      "Epoch [123/500], lter [8/9], Loss: 0.008777\n",
      "Epoch [124/500], lter [4/9], Loss: 0.010019\n",
      "Epoch [124/500], lter [8/9], Loss: 0.018109\n",
      "Epoch [125/500], lter [4/9], Loss: 0.016181\n",
      "Epoch [125/500], lter [8/9], Loss: 0.005383\n",
      "Epoch [126/500], lter [4/9], Loss: 0.010107\n",
      "Epoch [126/500], lter [8/9], Loss: 0.015161\n",
      "Epoch [127/500], lter [4/9], Loss: 0.015038\n",
      "Epoch [127/500], lter [8/9], Loss: 0.006912\n",
      "Epoch [128/500], lter [4/9], Loss: 0.006166\n",
      "Epoch [128/500], lter [8/9], Loss: 0.013778\n",
      "Epoch [129/500], lter [4/9], Loss: 0.006359\n",
      "Epoch [129/500], lter [8/9], Loss: 0.009771\n",
      "Epoch [130/500], lter [4/9], Loss: 0.009078\n",
      "Epoch [130/500], lter [8/9], Loss: 0.015737\n",
      "Epoch [131/500], lter [4/9], Loss: 0.005620\n",
      "Epoch [131/500], lter [8/9], Loss: 0.019346\n",
      "Epoch [132/500], lter [4/9], Loss: 0.015739\n",
      "Epoch [132/500], lter [8/9], Loss: 0.018979\n",
      "Epoch [133/500], lter [4/9], Loss: 0.007576\n",
      "Epoch [133/500], lter [8/9], Loss: 0.010034\n",
      "Epoch [134/500], lter [4/9], Loss: 0.007686\n",
      "Epoch [134/500], lter [8/9], Loss: 0.013734\n",
      "Epoch [135/500], lter [4/9], Loss: 0.010322\n",
      "Epoch [135/500], lter [8/9], Loss: 0.011905\n",
      "Epoch [136/500], lter [4/9], Loss: 0.017155\n",
      "Epoch [136/500], lter [8/9], Loss: 0.010951\n",
      "Epoch [137/500], lter [4/9], Loss: 0.010210\n",
      "Epoch [137/500], lter [8/9], Loss: 0.008128\n",
      "Epoch [138/500], lter [4/9], Loss: 0.014806\n",
      "Epoch [138/500], lter [8/9], Loss: 0.015739\n",
      "Epoch [139/500], lter [4/9], Loss: 0.010095\n",
      "Epoch [139/500], lter [8/9], Loss: 0.011000\n",
      "Epoch [140/500], lter [4/9], Loss: 0.007321\n",
      "Epoch [140/500], lter [8/9], Loss: 0.018366\n",
      "Epoch [141/500], lter [4/9], Loss: 0.008572\n",
      "Epoch [141/500], lter [8/9], Loss: 0.016927\n",
      "Epoch [142/500], lter [4/9], Loss: 0.004627\n",
      "Epoch [142/500], lter [8/9], Loss: 0.007696\n",
      "Epoch [143/500], lter [4/9], Loss: 0.010664\n",
      "Epoch [143/500], lter [8/9], Loss: 0.008907\n",
      "Epoch [144/500], lter [4/9], Loss: 0.010544\n",
      "Epoch [144/500], lter [8/9], Loss: 0.013948\n",
      "Epoch [145/500], lter [4/9], Loss: 0.012738\n",
      "Epoch [145/500], lter [8/9], Loss: 0.018209\n",
      "Epoch [146/500], lter [4/9], Loss: 0.006950\n",
      "Epoch [146/500], lter [8/9], Loss: 0.008778\n",
      "Epoch [147/500], lter [4/9], Loss: 0.011026\n",
      "Epoch [147/500], lter [8/9], Loss: 0.009984\n",
      "Epoch [148/500], lter [4/9], Loss: 0.010004\n",
      "Epoch [148/500], lter [8/9], Loss: 0.003211\n",
      "Epoch [149/500], lter [4/9], Loss: 0.010221\n",
      "Epoch [149/500], lter [8/9], Loss: 0.011636\n",
      "Epoch [150/500], lter [4/9], Loss: 0.013382\n",
      "Epoch [150/500], lter [8/9], Loss: 0.011456\n",
      "Epoch [151/500], lter [4/9], Loss: 0.007923\n",
      "Epoch [151/500], lter [8/9], Loss: 0.009477\n",
      "Epoch [152/500], lter [4/9], Loss: 0.007035\n",
      "Epoch [152/500], lter [8/9], Loss: 0.004544\n",
      "Epoch [153/500], lter [4/9], Loss: 0.013111\n",
      "Epoch [153/500], lter [8/9], Loss: 0.010762\n",
      "Epoch [154/500], lter [4/9], Loss: 0.014851\n",
      "Epoch [154/500], lter [8/9], Loss: 0.005665\n",
      "Epoch [155/500], lter [4/9], Loss: 0.010455\n",
      "Epoch [155/500], lter [8/9], Loss: 0.009654\n",
      "Epoch [156/500], lter [4/9], Loss: 0.008040\n",
      "Epoch [156/500], lter [8/9], Loss: 0.009290\n",
      "Epoch [157/500], lter [4/9], Loss: 0.013532\n",
      "Epoch [157/500], lter [8/9], Loss: 0.015387\n",
      "Epoch [158/500], lter [4/9], Loss: 0.011163\n",
      "Epoch [158/500], lter [8/9], Loss: 0.009728\n",
      "Epoch [159/500], lter [4/9], Loss: 0.016455\n",
      "Epoch [159/500], lter [8/9], Loss: 0.010491\n",
      "Epoch [160/500], lter [4/9], Loss: 0.010126\n",
      "Epoch [160/500], lter [8/9], Loss: 0.016566\n",
      "Epoch [161/500], lter [4/9], Loss: 0.015534\n",
      "Epoch [161/500], lter [8/9], Loss: 0.009894\n",
      "Epoch [162/500], lter [4/9], Loss: 0.009533\n",
      "Epoch [162/500], lter [8/9], Loss: 0.003561\n",
      "Epoch [163/500], lter [4/9], Loss: 0.020984\n",
      "Epoch [163/500], lter [8/9], Loss: 0.003384\n",
      "Epoch [164/500], lter [4/9], Loss: 0.005400\n",
      "Epoch [164/500], lter [8/9], Loss: 0.008481\n",
      "Epoch [165/500], lter [4/9], Loss: 0.007271\n",
      "Epoch [165/500], lter [8/9], Loss: 0.004506\n",
      "Epoch [166/500], lter [4/9], Loss: 0.008869\n",
      "Epoch [166/500], lter [8/9], Loss: 0.005348\n",
      "Epoch [167/500], lter [4/9], Loss: 0.009522\n",
      "Epoch [167/500], lter [8/9], Loss: 0.004280\n",
      "Epoch [168/500], lter [4/9], Loss: 0.003827\n",
      "Epoch [168/500], lter [8/9], Loss: 0.006637\n",
      "Epoch [169/500], lter [4/9], Loss: 0.008542\n",
      "Epoch [169/500], lter [8/9], Loss: 0.014531\n",
      "Epoch [170/500], lter [4/9], Loss: 0.010525\n",
      "Epoch [170/500], lter [8/9], Loss: 0.005840\n",
      "Epoch [171/500], lter [4/9], Loss: 0.008413\n",
      "Epoch [171/500], lter [8/9], Loss: 0.005466\n",
      "Epoch [172/500], lter [4/9], Loss: 0.007127\n",
      "Epoch [172/500], lter [8/9], Loss: 0.005833\n",
      "Epoch [173/500], lter [4/9], Loss: 0.015569\n",
      "Epoch [173/500], lter [8/9], Loss: 0.018719\n",
      "Epoch [174/500], lter [4/9], Loss: 0.010053\n",
      "Epoch [174/500], lter [8/9], Loss: 0.006584\n",
      "Epoch [175/500], lter [4/9], Loss: 0.006764\n",
      "Epoch [175/500], lter [8/9], Loss: 0.006986\n",
      "Epoch [176/500], lter [4/9], Loss: 0.008247\n",
      "Epoch [176/500], lter [8/9], Loss: 0.006869\n",
      "Epoch [177/500], lter [4/9], Loss: 0.006796\n",
      "Epoch [177/500], lter [8/9], Loss: 0.011039\n",
      "Epoch [178/500], lter [4/9], Loss: 0.007303\n",
      "Epoch [178/500], lter [8/9], Loss: 0.012487\n",
      "Epoch [179/500], lter [4/9], Loss: 0.006372\n",
      "Epoch [179/500], lter [8/9], Loss: 0.008163\n",
      "Epoch [180/500], lter [4/9], Loss: 0.011024\n",
      "Epoch [180/500], lter [8/9], Loss: 0.005494\n",
      "Epoch [181/500], lter [4/9], Loss: 0.008733\n",
      "Epoch [181/500], lter [8/9], Loss: 0.008040\n",
      "Epoch [182/500], lter [4/9], Loss: 0.004930\n",
      "Epoch [182/500], lter [8/9], Loss: 0.005978\n",
      "Epoch [183/500], lter [4/9], Loss: 0.005203\n",
      "Epoch [183/500], lter [8/9], Loss: 0.013800\n",
      "Epoch [184/500], lter [4/9], Loss: 0.011996\n",
      "Epoch [184/500], lter [8/9], Loss: 0.009522\n",
      "Epoch [185/500], lter [4/9], Loss: 0.007830\n",
      "Epoch [185/500], lter [8/9], Loss: 0.009623\n",
      "Epoch [186/500], lter [4/9], Loss: 0.010474\n",
      "Epoch [186/500], lter [8/9], Loss: 0.006375\n",
      "Epoch [187/500], lter [4/9], Loss: 0.009052\n",
      "Epoch [187/500], lter [8/9], Loss: 0.017369\n",
      "Epoch [188/500], lter [4/9], Loss: 0.005168\n",
      "Epoch [188/500], lter [8/9], Loss: 0.007025\n",
      "Epoch [189/500], lter [4/9], Loss: 0.008885\n",
      "Epoch [189/500], lter [8/9], Loss: 0.006253\n",
      "Epoch [190/500], lter [4/9], Loss: 0.007676\n",
      "Epoch [190/500], lter [8/9], Loss: 0.007263\n",
      "Epoch [191/500], lter [4/9], Loss: 0.005541\n",
      "Epoch [191/500], lter [8/9], Loss: 0.013544\n",
      "Epoch [192/500], lter [4/9], Loss: 0.012506\n",
      "Epoch [192/500], lter [8/9], Loss: 0.006985\n",
      "Epoch [193/500], lter [4/9], Loss: 0.005790\n",
      "Epoch [193/500], lter [8/9], Loss: 0.015411\n",
      "Epoch [194/500], lter [4/9], Loss: 0.011753\n",
      "Epoch [194/500], lter [8/9], Loss: 0.013259\n",
      "Epoch [195/500], lter [4/9], Loss: 0.006098\n",
      "Epoch [195/500], lter [8/9], Loss: 0.005096\n",
      "Epoch [196/500], lter [4/9], Loss: 0.004580\n",
      "Epoch [196/500], lter [8/9], Loss: 0.012950\n",
      "Epoch [197/500], lter [4/9], Loss: 0.006946\n",
      "Epoch [197/500], lter [8/9], Loss: 0.007807\n",
      "Epoch [198/500], lter [4/9], Loss: 0.006105\n",
      "Epoch [198/500], lter [8/9], Loss: 0.005083\n",
      "Epoch [199/500], lter [4/9], Loss: 0.011542\n",
      "Epoch [199/500], lter [8/9], Loss: 0.010900\n",
      "Epoch [200/500], lter [4/9], Loss: 0.004143\n",
      "Epoch [200/500], lter [8/9], Loss: 0.007732\n",
      "Epoch [201/500], lter [4/9], Loss: 0.009283\n",
      "Epoch [201/500], lter [8/9], Loss: 0.005387\n",
      "Epoch [202/500], lter [4/9], Loss: 0.008719\n",
      "Epoch [202/500], lter [8/9], Loss: 0.007655\n",
      "Epoch [203/500], lter [4/9], Loss: 0.014154\n",
      "Epoch [203/500], lter [8/9], Loss: 0.002886\n",
      "Epoch [204/500], lter [4/9], Loss: 0.003985\n",
      "Epoch [204/500], lter [8/9], Loss: 0.006622\n",
      "Epoch [205/500], lter [4/9], Loss: 0.008621\n",
      "Epoch [205/500], lter [8/9], Loss: 0.011631\n",
      "Epoch [206/500], lter [4/9], Loss: 0.008443\n",
      "Epoch [206/500], lter [8/9], Loss: 0.003955\n",
      "Epoch [207/500], lter [4/9], Loss: 0.011437\n",
      "Epoch [207/500], lter [8/9], Loss: 0.006574\n",
      "Epoch [208/500], lter [4/9], Loss: 0.004443\n",
      "Epoch [208/500], lter [8/9], Loss: 0.006552\n",
      "Epoch [209/500], lter [4/9], Loss: 0.007965\n",
      "Epoch [209/500], lter [8/9], Loss: 0.009875\n",
      "Epoch [210/500], lter [4/9], Loss: 0.009086\n",
      "Epoch [210/500], lter [8/9], Loss: 0.008226\n",
      "Epoch [211/500], lter [4/9], Loss: 0.007037\n",
      "Epoch [211/500], lter [8/9], Loss: 0.005460\n",
      "Epoch [212/500], lter [4/9], Loss: 0.003059\n",
      "Epoch [212/500], lter [8/9], Loss: 0.010860\n",
      "Epoch [213/500], lter [4/9], Loss: 0.006836\n",
      "Epoch [213/500], lter [8/9], Loss: 0.008680\n",
      "Epoch [214/500], lter [4/9], Loss: 0.004484\n",
      "Epoch [214/500], lter [8/9], Loss: 0.005905\n",
      "Epoch [215/500], lter [4/9], Loss: 0.010589\n",
      "Epoch [215/500], lter [8/9], Loss: 0.010391\n",
      "Epoch [216/500], lter [4/9], Loss: 0.009522\n",
      "Epoch [216/500], lter [8/9], Loss: 0.006843\n",
      "Epoch [217/500], lter [4/9], Loss: 0.009062\n",
      "Epoch [217/500], lter [8/9], Loss: 0.004518\n",
      "Epoch [218/500], lter [4/9], Loss: 0.012631\n",
      "Epoch [218/500], lter [8/9], Loss: 0.009125\n",
      "Epoch [219/500], lter [4/9], Loss: 0.011894\n",
      "Epoch [219/500], lter [8/9], Loss: 0.012779\n",
      "Epoch [220/500], lter [4/9], Loss: 0.005975\n",
      "Epoch [220/500], lter [8/9], Loss: 0.008720\n",
      "Epoch [221/500], lter [4/9], Loss: 0.007445\n",
      "Epoch [221/500], lter [8/9], Loss: 0.004147\n",
      "Epoch [222/500], lter [4/9], Loss: 0.009710\n",
      "Epoch [222/500], lter [8/9], Loss: 0.006919\n",
      "Epoch [223/500], lter [4/9], Loss: 0.003553\n",
      "Epoch [223/500], lter [8/9], Loss: 0.010893\n",
      "Epoch [224/500], lter [4/9], Loss: 0.006930\n",
      "Epoch [224/500], lter [8/9], Loss: 0.008898\n",
      "Epoch [225/500], lter [4/9], Loss: 0.005389\n",
      "Epoch [225/500], lter [8/9], Loss: 0.006054\n",
      "Epoch [226/500], lter [4/9], Loss: 0.003994\n",
      "Epoch [226/500], lter [8/9], Loss: 0.003923\n",
      "Epoch [227/500], lter [4/9], Loss: 0.008157\n",
      "Epoch [227/500], lter [8/9], Loss: 0.007563\n",
      "Epoch [228/500], lter [4/9], Loss: 0.006762\n",
      "Epoch [228/500], lter [8/9], Loss: 0.007079\n",
      "Epoch [229/500], lter [4/9], Loss: 0.004160\n",
      "Epoch [229/500], lter [8/9], Loss: 0.008248\n",
      "Epoch [230/500], lter [4/9], Loss: 0.006738\n",
      "Epoch [230/500], lter [8/9], Loss: 0.003903\n",
      "Epoch [231/500], lter [4/9], Loss: 0.005595\n",
      "Epoch [231/500], lter [8/9], Loss: 0.012737\n",
      "Epoch [232/500], lter [4/9], Loss: 0.017309\n",
      "Epoch [232/500], lter [8/9], Loss: 0.012693\n",
      "Epoch [233/500], lter [4/9], Loss: 0.010681\n",
      "Epoch [233/500], lter [8/9], Loss: 0.013276\n",
      "Epoch [234/500], lter [4/9], Loss: 0.008552\n",
      "Epoch [234/500], lter [8/9], Loss: 0.016660\n",
      "Epoch [235/500], lter [4/9], Loss: 0.008439\n",
      "Epoch [235/500], lter [8/9], Loss: 0.012133\n",
      "Epoch [236/500], lter [4/9], Loss: 0.015903\n",
      "Epoch [236/500], lter [8/9], Loss: 0.007201\n",
      "Epoch [237/500], lter [4/9], Loss: 0.003591\n",
      "Epoch [237/500], lter [8/9], Loss: 0.013586\n",
      "Epoch [238/500], lter [4/9], Loss: 0.009603\n",
      "Epoch [238/500], lter [8/9], Loss: 0.006967\n",
      "Epoch [239/500], lter [4/9], Loss: 0.008357\n",
      "Epoch [239/500], lter [8/9], Loss: 0.006009\n",
      "Epoch [240/500], lter [4/9], Loss: 0.006593\n",
      "Epoch [240/500], lter [8/9], Loss: 0.009577\n",
      "Epoch [241/500], lter [4/9], Loss: 0.009112\n",
      "Epoch [241/500], lter [8/9], Loss: 0.007324\n",
      "Epoch [242/500], lter [4/9], Loss: 0.007964\n",
      "Epoch [242/500], lter [8/9], Loss: 0.004310\n",
      "Epoch [243/500], lter [4/9], Loss: 0.008305\n",
      "Epoch [243/500], lter [8/9], Loss: 0.007155\n",
      "Epoch [244/500], lter [4/9], Loss: 0.007749\n",
      "Epoch [244/500], lter [8/9], Loss: 0.005410\n",
      "Epoch [245/500], lter [4/9], Loss: 0.011562\n",
      "Epoch [245/500], lter [8/9], Loss: 0.009925\n",
      "Epoch [246/500], lter [4/9], Loss: 0.005383\n",
      "Epoch [246/500], lter [8/9], Loss: 0.002944\n",
      "Epoch [247/500], lter [4/9], Loss: 0.002388\n",
      "Epoch [247/500], lter [8/9], Loss: 0.008617\n",
      "Epoch [248/500], lter [4/9], Loss: 0.007361\n",
      "Epoch [248/500], lter [8/9], Loss: 0.004815\n",
      "Epoch [249/500], lter [4/9], Loss: 0.012335\n",
      "Epoch [249/500], lter [8/9], Loss: 0.012493\n",
      "Epoch [250/500], lter [4/9], Loss: 0.009847\n",
      "Epoch [250/500], lter [8/9], Loss: 0.004841\n",
      "Epoch [251/500], lter [4/9], Loss: 0.007637\n",
      "Epoch [251/500], lter [8/9], Loss: 0.010886\n",
      "Epoch [252/500], lter [4/9], Loss: 0.008918\n",
      "Epoch [252/500], lter [8/9], Loss: 0.006060\n",
      "Epoch [253/500], lter [4/9], Loss: 0.008575\n",
      "Epoch [253/500], lter [8/9], Loss: 0.006653\n",
      "Epoch [254/500], lter [4/9], Loss: 0.014011\n",
      "Epoch [254/500], lter [8/9], Loss: 0.011228\n",
      "Epoch [255/500], lter [4/9], Loss: 0.010097\n",
      "Epoch [255/500], lter [8/9], Loss: 0.008243\n",
      "Epoch [256/500], lter [4/9], Loss: 0.006904\n",
      "Epoch [256/500], lter [8/9], Loss: 0.004516\n",
      "Epoch [257/500], lter [4/9], Loss: 0.003731\n",
      "Epoch [257/500], lter [8/9], Loss: 0.006897\n",
      "Epoch [258/500], lter [4/9], Loss: 0.005664\n",
      "Epoch [258/500], lter [8/9], Loss: 0.002613\n",
      "Epoch [259/500], lter [4/9], Loss: 0.008472\n",
      "Epoch [259/500], lter [8/9], Loss: 0.003875\n",
      "Epoch [260/500], lter [4/9], Loss: 0.006004\n",
      "Epoch [260/500], lter [8/9], Loss: 0.007483\n",
      "Epoch [261/500], lter [4/9], Loss: 0.008283\n",
      "Epoch [261/500], lter [8/9], Loss: 0.008684\n",
      "Epoch [262/500], lter [4/9], Loss: 0.005623\n",
      "Epoch [262/500], lter [8/9], Loss: 0.007158\n",
      "Epoch [263/500], lter [4/9], Loss: 0.014037\n",
      "Epoch [263/500], lter [8/9], Loss: 0.007346\n",
      "Epoch [264/500], lter [4/9], Loss: 0.005635\n",
      "Epoch [264/500], lter [8/9], Loss: 0.002333\n",
      "Epoch [265/500], lter [4/9], Loss: 0.003306\n",
      "Epoch [265/500], lter [8/9], Loss: 0.004820\n",
      "Epoch [266/500], lter [4/9], Loss: 0.006988\n",
      "Epoch [266/500], lter [8/9], Loss: 0.012240\n",
      "Epoch [267/500], lter [4/9], Loss: 0.008469\n",
      "Epoch [267/500], lter [8/9], Loss: 0.004289\n",
      "Epoch [268/500], lter [4/9], Loss: 0.010370\n",
      "Epoch [268/500], lter [8/9], Loss: 0.007583\n",
      "Epoch [269/500], lter [4/9], Loss: 0.012187\n",
      "Epoch [269/500], lter [8/9], Loss: 0.003180\n",
      "Epoch [270/500], lter [4/9], Loss: 0.006169\n",
      "Epoch [270/500], lter [8/9], Loss: 0.007198\n",
      "Epoch [271/500], lter [4/9], Loss: 0.010245\n",
      "Epoch [271/500], lter [8/9], Loss: 0.006967\n",
      "Epoch [272/500], lter [4/9], Loss: 0.005360\n",
      "Epoch [272/500], lter [8/9], Loss: 0.008307\n",
      "Epoch [273/500], lter [4/9], Loss: 0.004381\n",
      "Epoch [273/500], lter [8/9], Loss: 0.004925\n",
      "Epoch [274/500], lter [4/9], Loss: 0.007643\n",
      "Epoch [274/500], lter [8/9], Loss: 0.008310\n",
      "Epoch [275/500], lter [4/9], Loss: 0.002243\n",
      "Epoch [275/500], lter [8/9], Loss: 0.007525\n",
      "Epoch [276/500], lter [4/9], Loss: 0.005772\n",
      "Epoch [276/500], lter [8/9], Loss: 0.006090\n",
      "Epoch [277/500], lter [4/9], Loss: 0.003066\n",
      "Epoch [277/500], lter [8/9], Loss: 0.006899\n",
      "Epoch [278/500], lter [4/9], Loss: 0.006276\n",
      "Epoch [278/500], lter [8/9], Loss: 0.004137\n",
      "Epoch [279/500], lter [4/9], Loss: 0.007589\n",
      "Epoch [279/500], lter [8/9], Loss: 0.005626\n",
      "Epoch [280/500], lter [4/9], Loss: 0.011017\n",
      "Epoch [280/500], lter [8/9], Loss: 0.006865\n",
      "Epoch [281/500], lter [4/9], Loss: 0.007045\n",
      "Epoch [281/500], lter [8/9], Loss: 0.003534\n",
      "Epoch [282/500], lter [4/9], Loss: 0.009304\n",
      "Epoch [282/500], lter [8/9], Loss: 0.009402\n",
      "Epoch [283/500], lter [4/9], Loss: 0.004328\n",
      "Epoch [283/500], lter [8/9], Loss: 0.003923\n",
      "Epoch [284/500], lter [4/9], Loss: 0.010094\n",
      "Epoch [284/500], lter [8/9], Loss: 0.004663\n",
      "Epoch [285/500], lter [4/9], Loss: 0.013150\n",
      "Epoch [285/500], lter [8/9], Loss: 0.007982\n",
      "Epoch [286/500], lter [4/9], Loss: 0.003001\n",
      "Epoch [286/500], lter [8/9], Loss: 0.010719\n",
      "Epoch [287/500], lter [4/9], Loss: 0.014329\n",
      "Epoch [287/500], lter [8/9], Loss: 0.003529\n",
      "Epoch [288/500], lter [4/9], Loss: 0.011812\n",
      "Epoch [288/500], lter [8/9], Loss: 0.004264\n",
      "Epoch [289/500], lter [4/9], Loss: 0.005863\n",
      "Epoch [289/500], lter [8/9], Loss: 0.013509\n",
      "Epoch [290/500], lter [4/9], Loss: 0.008997\n",
      "Epoch [290/500], lter [8/9], Loss: 0.006633\n",
      "Epoch [291/500], lter [4/9], Loss: 0.008138\n",
      "Epoch [291/500], lter [8/9], Loss: 0.010289\n",
      "Epoch [292/500], lter [4/9], Loss: 0.011262\n",
      "Epoch [292/500], lter [8/9], Loss: 0.008881\n",
      "Epoch [293/500], lter [4/9], Loss: 0.007268\n",
      "Epoch [293/500], lter [8/9], Loss: 0.008721\n",
      "Epoch [294/500], lter [4/9], Loss: 0.005414\n",
      "Epoch [294/500], lter [8/9], Loss: 0.008506\n",
      "Epoch [295/500], lter [4/9], Loss: 0.012647\n",
      "Epoch [295/500], lter [8/9], Loss: 0.005760\n",
      "Epoch [296/500], lter [4/9], Loss: 0.002389\n",
      "Epoch [296/500], lter [8/9], Loss: 0.003853\n",
      "Epoch [297/500], lter [4/9], Loss: 0.004347\n",
      "Epoch [297/500], lter [8/9], Loss: 0.011460\n",
      "Epoch [298/500], lter [4/9], Loss: 0.011881\n",
      "Epoch [298/500], lter [8/9], Loss: 0.005371\n",
      "Epoch [299/500], lter [4/9], Loss: 0.004784\n",
      "Epoch [299/500], lter [8/9], Loss: 0.015718\n",
      "Epoch [300/500], lter [4/9], Loss: 0.004662\n",
      "Epoch [300/500], lter [8/9], Loss: 0.005686\n",
      "Epoch [301/500], lter [4/9], Loss: 0.012514\n",
      "Epoch [301/500], lter [8/9], Loss: 0.003738\n",
      "Epoch [302/500], lter [4/9], Loss: 0.006087\n",
      "Epoch [302/500], lter [8/9], Loss: 0.006741\n",
      "Epoch [303/500], lter [4/9], Loss: 0.005523\n",
      "Epoch [303/500], lter [8/9], Loss: 0.009750\n",
      "Epoch [304/500], lter [4/9], Loss: 0.006954\n",
      "Epoch [304/500], lter [8/9], Loss: 0.005711\n",
      "Epoch [305/500], lter [4/9], Loss: 0.006068\n",
      "Epoch [305/500], lter [8/9], Loss: 0.006989\n",
      "Epoch [306/500], lter [4/9], Loss: 0.010769\n",
      "Epoch [306/500], lter [8/9], Loss: 0.010111\n",
      "Epoch [307/500], lter [4/9], Loss: 0.005834\n",
      "Epoch [307/500], lter [8/9], Loss: 0.009520\n",
      "Epoch [308/500], lter [4/9], Loss: 0.005429\n",
      "Epoch [308/500], lter [8/9], Loss: 0.003772\n",
      "Epoch [309/500], lter [4/9], Loss: 0.006651\n",
      "Epoch [309/500], lter [8/9], Loss: 0.010273\n",
      "Epoch [310/500], lter [4/9], Loss: 0.008246\n",
      "Epoch [310/500], lter [8/9], Loss: 0.003924\n",
      "Epoch [311/500], lter [4/9], Loss: 0.004980\n",
      "Epoch [311/500], lter [8/9], Loss: 0.005449\n",
      "Epoch [312/500], lter [4/9], Loss: 0.009993\n",
      "Epoch [312/500], lter [8/9], Loss: 0.003450\n",
      "Epoch [313/500], lter [4/9], Loss: 0.015338\n",
      "Epoch [313/500], lter [8/9], Loss: 0.003793\n",
      "Epoch [314/500], lter [4/9], Loss: 0.002261\n",
      "Epoch [314/500], lter [8/9], Loss: 0.012734\n",
      "Epoch [315/500], lter [4/9], Loss: 0.008050\n",
      "Epoch [315/500], lter [8/9], Loss: 0.009859\n",
      "Epoch [316/500], lter [4/9], Loss: 0.007943\n",
      "Epoch [316/500], lter [8/9], Loss: 0.006960\n",
      "Epoch [317/500], lter [4/9], Loss: 0.009777\n",
      "Epoch [317/500], lter [8/9], Loss: 0.010382\n",
      "Epoch [318/500], lter [4/9], Loss: 0.007203\n",
      "Epoch [318/500], lter [8/9], Loss: 0.004961\n",
      "Epoch [319/500], lter [4/9], Loss: 0.004706\n",
      "Epoch [319/500], lter [8/9], Loss: 0.003357\n",
      "Epoch [320/500], lter [4/9], Loss: 0.007184\n",
      "Epoch [320/500], lter [8/9], Loss: 0.003461\n",
      "Epoch [321/500], lter [4/9], Loss: 0.006873\n",
      "Epoch [321/500], lter [8/9], Loss: 0.009054\n",
      "Epoch [322/500], lter [4/9], Loss: 0.003836\n",
      "Epoch [322/500], lter [8/9], Loss: 0.002091\n",
      "Epoch [323/500], lter [4/9], Loss: 0.006697\n",
      "Epoch [323/500], lter [8/9], Loss: 0.011217\n",
      "Epoch [324/500], lter [4/9], Loss: 0.005435\n",
      "Epoch [324/500], lter [8/9], Loss: 0.004479\n",
      "Epoch [325/500], lter [4/9], Loss: 0.005500\n",
      "Epoch [325/500], lter [8/9], Loss: 0.006110\n",
      "Epoch [326/500], lter [4/9], Loss: 0.016895\n",
      "Epoch [326/500], lter [8/9], Loss: 0.006627\n",
      "Epoch [327/500], lter [4/9], Loss: 0.009148\n",
      "Epoch [327/500], lter [8/9], Loss: 0.002534\n",
      "Epoch [328/500], lter [4/9], Loss: 0.009125\n",
      "Epoch [328/500], lter [8/9], Loss: 0.010869\n",
      "Epoch [329/500], lter [4/9], Loss: 0.013197\n",
      "Epoch [329/500], lter [8/9], Loss: 0.006382\n",
      "Epoch [330/500], lter [4/9], Loss: 0.006344\n",
      "Epoch [330/500], lter [8/9], Loss: 0.006263\n",
      "Epoch [331/500], lter [4/9], Loss: 0.003611\n",
      "Epoch [331/500], lter [8/9], Loss: 0.010818\n",
      "Epoch [332/500], lter [4/9], Loss: 0.007314\n",
      "Epoch [332/500], lter [8/9], Loss: 0.009605\n",
      "Epoch [333/500], lter [4/9], Loss: 0.006438\n",
      "Epoch [333/500], lter [8/9], Loss: 0.009087\n",
      "Epoch [334/500], lter [4/9], Loss: 0.002160\n",
      "Epoch [334/500], lter [8/9], Loss: 0.002948\n",
      "Epoch [335/500], lter [4/9], Loss: 0.005307\n",
      "Epoch [335/500], lter [8/9], Loss: 0.003020\n",
      "Epoch [336/500], lter [4/9], Loss: 0.009164\n",
      "Epoch [336/500], lter [8/9], Loss: 0.002418\n",
      "Epoch [337/500], lter [4/9], Loss: 0.009996\n",
      "Epoch [337/500], lter [8/9], Loss: 0.004676\n",
      "Epoch [338/500], lter [4/9], Loss: 0.006005\n",
      "Epoch [338/500], lter [8/9], Loss: 0.004243\n",
      "Epoch [339/500], lter [4/9], Loss: 0.009628\n",
      "Epoch [339/500], lter [8/9], Loss: 0.004777\n",
      "Epoch [340/500], lter [4/9], Loss: 0.006736\n",
      "Epoch [340/500], lter [8/9], Loss: 0.010786\n",
      "Epoch [341/500], lter [4/9], Loss: 0.011131\n",
      "Epoch [341/500], lter [8/9], Loss: 0.003922\n",
      "Epoch [342/500], lter [4/9], Loss: 0.007742\n",
      "Epoch [342/500], lter [8/9], Loss: 0.001709\n",
      "Epoch [343/500], lter [4/9], Loss: 0.005491\n",
      "Epoch [343/500], lter [8/9], Loss: 0.007370\n",
      "Epoch [344/500], lter [4/9], Loss: 0.008940\n",
      "Epoch [344/500], lter [8/9], Loss: 0.009552\n",
      "Epoch [345/500], lter [4/9], Loss: 0.009172\n",
      "Epoch [345/500], lter [8/9], Loss: 0.008865\n",
      "Epoch [346/500], lter [4/9], Loss: 0.004770\n",
      "Epoch [346/500], lter [8/9], Loss: 0.007673\n",
      "Epoch [347/500], lter [4/9], Loss: 0.006107\n",
      "Epoch [347/500], lter [8/9], Loss: 0.005523\n",
      "Epoch [348/500], lter [4/9], Loss: 0.004570\n",
      "Epoch [348/500], lter [8/9], Loss: 0.005254\n",
      "Epoch [349/500], lter [4/9], Loss: 0.005277\n",
      "Epoch [349/500], lter [8/9], Loss: 0.003250\n",
      "Epoch [350/500], lter [4/9], Loss: 0.005958\n",
      "Epoch [350/500], lter [8/9], Loss: 0.004721\n",
      "Epoch [351/500], lter [4/9], Loss: 0.005728\n",
      "Epoch [351/500], lter [8/9], Loss: 0.002773\n",
      "Epoch [352/500], lter [4/9], Loss: 0.003438\n",
      "Epoch [352/500], lter [8/9], Loss: 0.005007\n",
      "Epoch [353/500], lter [4/9], Loss: 0.007667\n",
      "Epoch [353/500], lter [8/9], Loss: 0.009006\n",
      "Epoch [354/500], lter [4/9], Loss: 0.004856\n",
      "Epoch [354/500], lter [8/9], Loss: 0.003088\n",
      "Epoch [355/500], lter [4/9], Loss: 0.004853\n",
      "Epoch [355/500], lter [8/9], Loss: 0.011607\n",
      "Epoch [356/500], lter [4/9], Loss: 0.009619\n",
      "Epoch [356/500], lter [8/9], Loss: 0.003995\n",
      "Epoch [357/500], lter [4/9], Loss: 0.006287\n",
      "Epoch [357/500], lter [8/9], Loss: 0.003010\n",
      "Epoch [358/500], lter [4/9], Loss: 0.006414\n",
      "Epoch [358/500], lter [8/9], Loss: 0.004865\n",
      "Epoch [359/500], lter [4/9], Loss: 0.006814\n",
      "Epoch [359/500], lter [8/9], Loss: 0.011902\n",
      "Epoch [360/500], lter [4/9], Loss: 0.009855\n",
      "Epoch [360/500], lter [8/9], Loss: 0.008013\n",
      "Epoch [361/500], lter [4/9], Loss: 0.007358\n",
      "Epoch [361/500], lter [8/9], Loss: 0.015970\n",
      "Epoch [362/500], lter [4/9], Loss: 0.007598\n",
      "Epoch [362/500], lter [8/9], Loss: 0.008285\n",
      "Epoch [363/500], lter [4/9], Loss: 0.007937\n",
      "Epoch [363/500], lter [8/9], Loss: 0.009322\n",
      "Epoch [364/500], lter [4/9], Loss: 0.006624\n",
      "Epoch [364/500], lter [8/9], Loss: 0.011272\n",
      "Epoch [365/500], lter [4/9], Loss: 0.014744\n",
      "Epoch [365/500], lter [8/9], Loss: 0.006225\n",
      "Epoch [366/500], lter [4/9], Loss: 0.005001\n",
      "Epoch [366/500], lter [8/9], Loss: 0.008365\n",
      "Epoch [367/500], lter [4/9], Loss: 0.010404\n",
      "Epoch [367/500], lter [8/9], Loss: 0.006232\n",
      "Epoch [368/500], lter [4/9], Loss: 0.006467\n",
      "Epoch [368/500], lter [8/9], Loss: 0.005878\n",
      "Epoch [369/500], lter [4/9], Loss: 0.003069\n",
      "Epoch [369/500], lter [8/9], Loss: 0.012314\n",
      "Epoch [370/500], lter [4/9], Loss: 0.013775\n",
      "Epoch [370/500], lter [8/9], Loss: 0.008576\n",
      "Epoch [371/500], lter [4/9], Loss: 0.004735\n",
      "Epoch [371/500], lter [8/9], Loss: 0.008324\n",
      "Epoch [372/500], lter [4/9], Loss: 0.007068\n",
      "Epoch [372/500], lter [8/9], Loss: 0.004569\n",
      "Epoch [373/500], lter [4/9], Loss: 0.005820\n",
      "Epoch [373/500], lter [8/9], Loss: 0.006401\n",
      "Epoch [374/500], lter [4/9], Loss: 0.008355\n",
      "Epoch [374/500], lter [8/9], Loss: 0.010998\n",
      "Epoch [375/500], lter [4/9], Loss: 0.004224\n",
      "Epoch [375/500], lter [8/9], Loss: 0.009046\n",
      "Epoch [376/500], lter [4/9], Loss: 0.002745\n",
      "Epoch [376/500], lter [8/9], Loss: 0.008289\n",
      "Epoch [377/500], lter [4/9], Loss: 0.010096\n",
      "Epoch [377/500], lter [8/9], Loss: 0.007834\n",
      "Epoch [378/500], lter [4/9], Loss: 0.003953\n",
      "Epoch [378/500], lter [8/9], Loss: 0.016001\n",
      "Epoch [379/500], lter [4/9], Loss: 0.005091\n",
      "Epoch [379/500], lter [8/9], Loss: 0.001092\n",
      "Epoch [380/500], lter [4/9], Loss: 0.010934\n",
      "Epoch [380/500], lter [8/9], Loss: 0.015276\n",
      "Epoch [381/500], lter [4/9], Loss: 0.010042\n",
      "Epoch [381/500], lter [8/9], Loss: 0.006135\n",
      "Epoch [382/500], lter [4/9], Loss: 0.014755\n",
      "Epoch [382/500], lter [8/9], Loss: 0.004076\n",
      "Epoch [383/500], lter [4/9], Loss: 0.010124\n",
      "Epoch [383/500], lter [8/9], Loss: 0.006270\n",
      "Epoch [384/500], lter [4/9], Loss: 0.007440\n",
      "Epoch [384/500], lter [8/9], Loss: 0.002036\n",
      "Epoch [385/500], lter [4/9], Loss: 0.010906\n",
      "Epoch [385/500], lter [8/9], Loss: 0.009258\n",
      "Epoch [386/500], lter [4/9], Loss: 0.009366\n",
      "Epoch [386/500], lter [8/9], Loss: 0.004931\n",
      "Epoch [387/500], lter [4/9], Loss: 0.005076\n",
      "Epoch [387/500], lter [8/9], Loss: 0.007352\n",
      "Epoch [388/500], lter [4/9], Loss: 0.003457\n",
      "Epoch [388/500], lter [8/9], Loss: 0.003367\n",
      "Epoch [389/500], lter [4/9], Loss: 0.003473\n",
      "Epoch [389/500], lter [8/9], Loss: 0.007225\n",
      "Epoch [390/500], lter [4/9], Loss: 0.008126\n",
      "Epoch [390/500], lter [8/9], Loss: 0.008444\n",
      "Epoch [391/500], lter [4/9], Loss: 0.007953\n",
      "Epoch [391/500], lter [8/9], Loss: 0.006361\n",
      "Epoch [392/500], lter [4/9], Loss: 0.008031\n",
      "Epoch [392/500], lter [8/9], Loss: 0.011406\n",
      "Epoch [393/500], lter [4/9], Loss: 0.005756\n",
      "Epoch [393/500], lter [8/9], Loss: 0.004029\n",
      "Epoch [394/500], lter [4/9], Loss: 0.007134\n",
      "Epoch [394/500], lter [8/9], Loss: 0.004605\n",
      "Epoch [395/500], lter [4/9], Loss: 0.003642\n",
      "Epoch [395/500], lter [8/9], Loss: 0.008479\n",
      "Epoch [396/500], lter [4/9], Loss: 0.005021\n",
      "Epoch [396/500], lter [8/9], Loss: 0.004908\n",
      "Epoch [397/500], lter [4/9], Loss: 0.009285\n",
      "Epoch [397/500], lter [8/9], Loss: 0.005168\n",
      "Epoch [398/500], lter [4/9], Loss: 0.007675\n",
      "Epoch [398/500], lter [8/9], Loss: 0.009173\n",
      "Epoch [399/500], lter [4/9], Loss: 0.004422\n",
      "Epoch [399/500], lter [8/9], Loss: 0.002520\n",
      "Epoch [400/500], lter [4/9], Loss: 0.010160\n",
      "Epoch [400/500], lter [8/9], Loss: 0.003215\n",
      "Epoch [401/500], lter [4/9], Loss: 0.005854\n",
      "Epoch [401/500], lter [8/9], Loss: 0.004341\n",
      "Epoch [402/500], lter [4/9], Loss: 0.003898\n",
      "Epoch [402/500], lter [8/9], Loss: 0.002295\n",
      "Epoch [403/500], lter [4/9], Loss: 0.003289\n",
      "Epoch [403/500], lter [8/9], Loss: 0.012793\n",
      "Epoch [404/500], lter [4/9], Loss: 0.005543\n",
      "Epoch [404/500], lter [8/9], Loss: 0.005680\n",
      "Epoch [405/500], lter [4/9], Loss: 0.004547\n",
      "Epoch [405/500], lter [8/9], Loss: 0.005808\n",
      "Epoch [406/500], lter [4/9], Loss: 0.012849\n",
      "Epoch [406/500], lter [8/9], Loss: 0.004559\n",
      "Epoch [407/500], lter [4/9], Loss: 0.014586\n",
      "Epoch [407/500], lter [8/9], Loss: 0.009923\n",
      "Epoch [408/500], lter [4/9], Loss: 0.010601\n",
      "Epoch [408/500], lter [8/9], Loss: 0.002653\n",
      "Epoch [409/500], lter [4/9], Loss: 0.008079\n",
      "Epoch [409/500], lter [8/9], Loss: 0.007499\n",
      "Epoch [410/500], lter [4/9], Loss: 0.007316\n",
      "Epoch [410/500], lter [8/9], Loss: 0.003847\n",
      "Epoch [411/500], lter [4/9], Loss: 0.003438\n",
      "Epoch [411/500], lter [8/9], Loss: 0.010060\n",
      "Epoch [412/500], lter [4/9], Loss: 0.003484\n",
      "Epoch [412/500], lter [8/9], Loss: 0.008995\n",
      "Epoch [413/500], lter [4/9], Loss: 0.011045\n",
      "Epoch [413/500], lter [8/9], Loss: 0.010436\n",
      "Epoch [414/500], lter [4/9], Loss: 0.005004\n",
      "Epoch [414/500], lter [8/9], Loss: 0.004817\n",
      "Epoch [415/500], lter [4/9], Loss: 0.007023\n",
      "Epoch [415/500], lter [8/9], Loss: 0.004927\n",
      "Epoch [416/500], lter [4/9], Loss: 0.005436\n",
      "Epoch [416/500], lter [8/9], Loss: 0.004031\n",
      "Epoch [417/500], lter [4/9], Loss: 0.005934\n",
      "Epoch [417/500], lter [8/9], Loss: 0.008419\n",
      "Epoch [418/500], lter [4/9], Loss: 0.007230\n",
      "Epoch [418/500], lter [8/9], Loss: 0.010825\n",
      "Epoch [419/500], lter [4/9], Loss: 0.008239\n",
      "Epoch [419/500], lter [8/9], Loss: 0.007648\n",
      "Epoch [420/500], lter [4/9], Loss: 0.007490\n",
      "Epoch [420/500], lter [8/9], Loss: 0.005942\n",
      "Epoch [421/500], lter [4/9], Loss: 0.008331\n",
      "Epoch [421/500], lter [8/9], Loss: 0.003832\n",
      "Epoch [422/500], lter [4/9], Loss: 0.004010\n",
      "Epoch [422/500], lter [8/9], Loss: 0.010368\n",
      "Epoch [423/500], lter [4/9], Loss: 0.004203\n",
      "Epoch [423/500], lter [8/9], Loss: 0.001136\n",
      "Epoch [424/500], lter [4/9], Loss: 0.001613\n",
      "Epoch [424/500], lter [8/9], Loss: 0.006544\n",
      "Epoch [425/500], lter [4/9], Loss: 0.006304\n",
      "Epoch [425/500], lter [8/9], Loss: 0.004085\n",
      "Epoch [426/500], lter [4/9], Loss: 0.008911\n",
      "Epoch [426/500], lter [8/9], Loss: 0.006409\n",
      "Epoch [427/500], lter [4/9], Loss: 0.012213\n",
      "Epoch [427/500], lter [8/9], Loss: 0.011421\n",
      "Epoch [428/500], lter [4/9], Loss: 0.004137\n",
      "Epoch [428/500], lter [8/9], Loss: 0.004645\n",
      "Epoch [429/500], lter [4/9], Loss: 0.002025\n",
      "Epoch [429/500], lter [8/9], Loss: 0.006199\n",
      "Epoch [430/500], lter [4/9], Loss: 0.002466\n",
      "Epoch [430/500], lter [8/9], Loss: 0.014493\n",
      "Epoch [431/500], lter [4/9], Loss: 0.005786\n",
      "Epoch [431/500], lter [8/9], Loss: 0.009117\n",
      "Epoch [432/500], lter [4/9], Loss: 0.006607\n",
      "Epoch [432/500], lter [8/9], Loss: 0.008935\n",
      "Epoch [433/500], lter [4/9], Loss: 0.005762\n",
      "Epoch [433/500], lter [8/9], Loss: 0.003200\n",
      "Epoch [434/500], lter [4/9], Loss: 0.004076\n",
      "Epoch [434/500], lter [8/9], Loss: 0.002880\n",
      "Epoch [435/500], lter [4/9], Loss: 0.010954\n",
      "Epoch [435/500], lter [8/9], Loss: 0.006101\n",
      "Epoch [436/500], lter [4/9], Loss: 0.010987\n",
      "Epoch [436/500], lter [8/9], Loss: 0.005984\n",
      "Epoch [437/500], lter [4/9], Loss: 0.006215\n",
      "Epoch [437/500], lter [8/9], Loss: 0.010130\n",
      "Epoch [438/500], lter [4/9], Loss: 0.007380\n",
      "Epoch [438/500], lter [8/9], Loss: 0.005198\n",
      "Epoch [439/500], lter [4/9], Loss: 0.010465\n",
      "Epoch [439/500], lter [8/9], Loss: 0.002093\n",
      "Epoch [440/500], lter [4/9], Loss: 0.003490\n",
      "Epoch [440/500], lter [8/9], Loss: 0.002615\n",
      "Epoch [441/500], lter [4/9], Loss: 0.012646\n",
      "Epoch [441/500], lter [8/9], Loss: 0.008814\n",
      "Epoch [442/500], lter [4/9], Loss: 0.010055\n",
      "Epoch [442/500], lter [8/9], Loss: 0.009111\n",
      "Epoch [443/500], lter [4/9], Loss: 0.002337\n",
      "Epoch [443/500], lter [8/9], Loss: 0.007946\n",
      "Epoch [444/500], lter [4/9], Loss: 0.019056\n",
      "Epoch [444/500], lter [8/9], Loss: 0.006885\n",
      "Epoch [445/500], lter [4/9], Loss: 0.005079\n",
      "Epoch [445/500], lter [8/9], Loss: 0.019172\n",
      "Epoch [446/500], lter [4/9], Loss: 0.006957\n",
      "Epoch [446/500], lter [8/9], Loss: 0.004454\n",
      "Epoch [447/500], lter [4/9], Loss: 0.006808\n",
      "Epoch [447/500], lter [8/9], Loss: 0.010717\n",
      "Epoch [448/500], lter [4/9], Loss: 0.004866\n",
      "Epoch [448/500], lter [8/9], Loss: 0.015035\n",
      "Epoch [449/500], lter [4/9], Loss: 0.009161\n",
      "Epoch [449/500], lter [8/9], Loss: 0.003037\n",
      "Epoch [450/500], lter [4/9], Loss: 0.006978\n",
      "Epoch [450/500], lter [8/9], Loss: 0.010563\n",
      "Epoch [451/500], lter [4/9], Loss: 0.010077\n",
      "Epoch [451/500], lter [8/9], Loss: 0.005088\n",
      "Epoch [452/500], lter [4/9], Loss: 0.009875\n",
      "Epoch [452/500], lter [8/9], Loss: 0.010380\n",
      "Epoch [453/500], lter [4/9], Loss: 0.008201\n",
      "Epoch [453/500], lter [8/9], Loss: 0.005465\n",
      "Epoch [454/500], lter [4/9], Loss: 0.007265\n",
      "Epoch [454/500], lter [8/9], Loss: 0.003272\n",
      "Epoch [455/500], lter [4/9], Loss: 0.008009\n",
      "Epoch [455/500], lter [8/9], Loss: 0.007329\n",
      "Epoch [456/500], lter [4/9], Loss: 0.009593\n",
      "Epoch [456/500], lter [8/9], Loss: 0.004326\n",
      "Epoch [457/500], lter [4/9], Loss: 0.005086\n",
      "Epoch [457/500], lter [8/9], Loss: 0.005158\n",
      "Epoch [458/500], lter [4/9], Loss: 0.010307\n",
      "Epoch [458/500], lter [8/9], Loss: 0.008806\n",
      "Epoch [459/500], lter [4/9], Loss: 0.004212\n",
      "Epoch [459/500], lter [8/9], Loss: 0.009442\n",
      "Epoch [460/500], lter [4/9], Loss: 0.009996\n",
      "Epoch [460/500], lter [8/9], Loss: 0.004216\n",
      "Epoch [461/500], lter [4/9], Loss: 0.007668\n",
      "Epoch [461/500], lter [8/9], Loss: 0.006233\n",
      "Epoch [462/500], lter [4/9], Loss: 0.005313\n",
      "Epoch [462/500], lter [8/9], Loss: 0.006741\n",
      "Epoch [463/500], lter [4/9], Loss: 0.004958\n",
      "Epoch [463/500], lter [8/9], Loss: 0.004748\n",
      "Epoch [464/500], lter [4/9], Loss: 0.010409\n",
      "Epoch [464/500], lter [8/9], Loss: 0.006860\n",
      "Epoch [465/500], lter [4/9], Loss: 0.004602\n",
      "Epoch [465/500], lter [8/9], Loss: 0.009988\n",
      "Epoch [466/500], lter [4/9], Loss: 0.008097\n",
      "Epoch [466/500], lter [8/9], Loss: 0.007707\n",
      "Epoch [467/500], lter [4/9], Loss: 0.004692\n",
      "Epoch [467/500], lter [8/9], Loss: 0.002607\n",
      "Epoch [468/500], lter [4/9], Loss: 0.002211\n",
      "Epoch [468/500], lter [8/9], Loss: 0.003823\n",
      "Epoch [469/500], lter [4/9], Loss: 0.004414\n",
      "Epoch [469/500], lter [8/9], Loss: 0.014738\n",
      "Epoch [470/500], lter [4/9], Loss: 0.010099\n",
      "Epoch [470/500], lter [8/9], Loss: 0.008185\n",
      "Epoch [471/500], lter [4/9], Loss: 0.003385\n",
      "Epoch [471/500], lter [8/9], Loss: 0.002513\n",
      "Epoch [472/500], lter [4/9], Loss: 0.006189\n",
      "Epoch [472/500], lter [8/9], Loss: 0.005793\n",
      "Epoch [473/500], lter [4/9], Loss: 0.012397\n",
      "Epoch [473/500], lter [8/9], Loss: 0.007946\n",
      "Epoch [474/500], lter [4/9], Loss: 0.009126\n",
      "Epoch [474/500], lter [8/9], Loss: 0.003891\n",
      "Epoch [475/500], lter [4/9], Loss: 0.008997\n",
      "Epoch [475/500], lter [8/9], Loss: 0.005459\n",
      "Epoch [476/500], lter [4/9], Loss: 0.013560\n",
      "Epoch [476/500], lter [8/9], Loss: 0.002807\n",
      "Epoch [477/500], lter [4/9], Loss: 0.001703\n",
      "Epoch [477/500], lter [8/9], Loss: 0.006281\n",
      "Epoch [478/500], lter [4/9], Loss: 0.009052\n",
      "Epoch [478/500], lter [8/9], Loss: 0.004820\n",
      "Epoch [479/500], lter [4/9], Loss: 0.005218\n",
      "Epoch [479/500], lter [8/9], Loss: 0.005740\n",
      "Epoch [480/500], lter [4/9], Loss: 0.009976\n",
      "Epoch [480/500], lter [8/9], Loss: 0.007957\n",
      "Epoch [481/500], lter [4/9], Loss: 0.010603\n",
      "Epoch [481/500], lter [8/9], Loss: 0.009054\n",
      "Epoch [482/500], lter [4/9], Loss: 0.004520\n",
      "Epoch [482/500], lter [8/9], Loss: 0.012886\n",
      "Epoch [483/500], lter [4/9], Loss: 0.008397\n",
      "Epoch [483/500], lter [8/9], Loss: 0.001738\n",
      "Epoch [484/500], lter [4/9], Loss: 0.007585\n",
      "Epoch [484/500], lter [8/9], Loss: 0.007065\n",
      "Epoch [485/500], lter [4/9], Loss: 0.008033\n",
      "Epoch [485/500], lter [8/9], Loss: 0.005655\n",
      "Epoch [486/500], lter [4/9], Loss: 0.017038\n",
      "Epoch [486/500], lter [8/9], Loss: 0.006982\n",
      "Epoch [487/500], lter [4/9], Loss: 0.002997\n",
      "Epoch [487/500], lter [8/9], Loss: 0.002212\n",
      "Epoch [488/500], lter [4/9], Loss: 0.001858\n",
      "Epoch [488/500], lter [8/9], Loss: 0.012889\n",
      "Epoch [489/500], lter [4/9], Loss: 0.009059\n",
      "Epoch [489/500], lter [8/9], Loss: 0.003142\n",
      "Epoch [490/500], lter [4/9], Loss: 0.001411\n",
      "Epoch [490/500], lter [8/9], Loss: 0.005789\n",
      "Epoch [491/500], lter [4/9], Loss: 0.008175\n",
      "Epoch [491/500], lter [8/9], Loss: 0.003810\n",
      "Epoch [492/500], lter [4/9], Loss: 0.011550\n",
      "Epoch [492/500], lter [8/9], Loss: 0.004899\n",
      "Epoch [493/500], lter [4/9], Loss: 0.006110\n",
      "Epoch [493/500], lter [8/9], Loss: 0.009070\n",
      "Epoch [494/500], lter [4/9], Loss: 0.013379\n",
      "Epoch [494/500], lter [8/9], Loss: 0.007712\n",
      "Epoch [495/500], lter [4/9], Loss: 0.004876\n",
      "Epoch [495/500], lter [8/9], Loss: 0.005735\n",
      "Epoch [496/500], lter [4/9], Loss: 0.005700\n",
      "Epoch [496/500], lter [8/9], Loss: 0.008438\n",
      "Epoch [497/500], lter [4/9], Loss: 0.010896\n",
      "Epoch [497/500], lter [8/9], Loss: 0.011865\n",
      "Epoch [498/500], lter [4/9], Loss: 0.012888\n",
      "Epoch [498/500], lter [8/9], Loss: 0.009128\n",
      "Epoch [499/500], lter [4/9], Loss: 0.004453\n",
      "Epoch [499/500], lter [8/9], Loss: 0.007969\n",
      "Epoch [500/500], lter [4/9], Loss: 0.002225\n",
      "Epoch [500/500], lter [8/9], Loss: 0.006584\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_list = []\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        \n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        reconst_data = model(batch_data)\n",
    "        cost = loss(reconst_data, batch_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        if (i+1) % (total_batch//2) == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d], Loss: %.6f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.item()))\n",
    "            \n",
    "        # early stopping rule 1 : MSE < 1e-06\n",
    "        if cost.item() < 1e-06 :\n",
    "            early_stop = True\n",
    "            break\n",
    "            \n",
    "#         early stopping rule 2 : simple moving average of length 5\n",
    "#         sometimes it doesn't work well.\n",
    "#         if len(cost_list) > 5 :\n",
    "#            if cost.item() > np.mean(cost_list[-5:]):\n",
    "#                early_stop = True\n",
    "#                break\n",
    "                \n",
    "        cost_list.append(cost.item())\n",
    "\n",
    "    if early_stop :\n",
    "        break\n",
    "        \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE_SUM : 1.110761877939202\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "filled_data = model(missed_data.to(device))\n",
    "filled_data = filled_data.cpu().detach().numpy()\n",
    "\n",
    "rmse_sum = 0\n",
    "\n",
    "for i in range(cols) :\n",
    "    if mask[:,i].sum() > 0 :\n",
    "        y_actual = test_data[:,i][mask[:,i]]\n",
    "        y_predicted = filled_data[:,i][mask[:,i]]\n",
    "\n",
    "        rmse = sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "        rmse_sum += rmse\n",
    "    \n",
    "print(\"RMSE_SUM :\", rmse_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22215237558784037"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_data, filled_data, sample_weight=mask.astype(int), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4695052853198908"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_data, missed_data, sample_weight=mask.astype(int), squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
